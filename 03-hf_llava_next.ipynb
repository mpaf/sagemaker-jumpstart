{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8739f07c-c846-41af-a8aa-ea0940bb9b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers --upgrade\n",
    "!pip uninstall bitsandbytes -y\n",
    "!pip install bitsandbytes --upgrade\n",
    "!pip install sagemaker --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58382dc3",
   "metadata": {},
   "source": [
    "## Run Image Reasoning LLaVA-NeXT model Locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237da4ce",
   "metadata": {},
   "source": [
    "If running on a g4dn instance type and you require to quantize the model to fit in the GPU, you might need to run:\n",
    "\n",
    "```bash\n",
    "cd /opt/conda/lib/python3.10/site-packages/bitsandbytes/\n",
    "cp libbitsandbytes_cuda117.so libbitsandbytes_cpu.so\n",
    "```\n",
    "\n",
    "and restart this kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552f4c64-0e0f-4542-8022-5fa8d9a0fffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "processor = LlavaNextProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\")\n",
    "\n",
    "model = LlavaNextForConditionalGeneration.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\", torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True, device_map=\"cuda:0\", quantization_config=BitsAndBytesConfig(load_in_4bit=True)) \n",
    "#model.to(\"cuda:0\")\n",
    "\n",
    "# prepare image and text prompt, using the appropriate prompt template\n",
    "url = \"https://github.com/haotian-liu/LLaVA/blob/1a91fc274d7c35a9b50b3cb29c4247ae5837ce39/images/llava_v1_5_radar.jpg?raw=true\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "prompt = \"[INST] <image>\\nWhat is shown in this image? [/INST]\"\n",
    "\n",
    "inputs = processor(prompt, image, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "\n",
    "# autoregressively complete prompt\n",
    "output = model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "print(processor.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90daf93-4ccd-481b-982a-91555f2b7c95",
   "metadata": {},
   "source": [
    "## Deploy to Amazon SageMaker real-time endpoint with custom inference code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b733882-8d3b-4e23-b297-dc96f69fa8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# package_code\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "!tar czvf model.tar.gz code/\n",
    "code_artifact = sagemaker_session.upload_data(\"model.tar.gz\", bucket, 'model_artifacts')\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {code_artifact}\")\n",
    "!rm model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32390ea-e4da-4904-b9de-5ccd4bd48f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "HF_MODEL_ID = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n",
    "\n",
    "hub = {\n",
    "    \"HF_MODEL_ID\": HF_MODEL_ID,\n",
    "    \"HF_TASK\": \"visual-question-answering\",\n",
    "}\n",
    "\n",
    "# creating SageMaker Model\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    name='llava-v16',\n",
    "    transformers_version=\"4.37\",\n",
    "    model_data=code_artifact,\n",
    "    pytorch_version=\"2.1\",\n",
    "    py_version=\"py310\",\n",
    "    #env=hub,\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "predictor = huggingface_model.deploy(\n",
    "    instance_type='ml.g5.xlarge',\n",
    "    initial_instance_count=1,\n",
    "    endpoint_name='sm-endpoint',\n",
    "    role=role,\n",
    "    tags={},\n",
    "    model_data_download_timeout=3600,\n",
    "    container_startup_health_check_timeout=1200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972cab5b-c738-4713-bd83-96b0f13cef2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Inference\n",
    "inputs = {\n",
    "    \"inputs\": {\n",
    "        \"image\": 'http://images.cocodataset.org/val2017/000000039769.jpg',\n",
    "        \"question\": \"What is the species of the cat depicted in the picture?\"\n",
    "    }\n",
    "}\n",
    "\n",
    "response = predictor.predict(inputs)\n",
    "print(response[0]['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d02d29f",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d5f4d8-024e-416b-bba0-93f5b2e0f582",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()\n",
    "huggingface_model.delete_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
